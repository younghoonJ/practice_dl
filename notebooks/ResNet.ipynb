{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06aca601-88b7-486b-b2fc-12cc74b65f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74b3b89f-a60e-449c-8282-73c1697c5b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb4a328-0a5e-463f-822e-810b96a13cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, dilation=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            out += self.downsample(x)\n",
    "        else:\n",
    "            out += x\n",
    "        \n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def make_layer(block, in_channels, out_channels, n_blokcs=2, stride=1):\n",
    "    if stride != 1:\n",
    "        downsample = nn.Sequential(conv1x1(in_channels, out_channels, stride))\n",
    "    else:\n",
    "        downsample = None\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(block(in_channels, out_channels, stride, downsample))\n",
    "    for _ in range(1, n_blokcs):\n",
    "        layers.append(block(out_channels, out_channels))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb97fc46-d5fd-433f-9c81-b381085a174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(nn.Module):\n",
    "    def __init__(self, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = make_layer(BasicBlock, 64, 64)\n",
    "        self.layer2 = make_layer(BasicBlock, 64, 128, stride=2)\n",
    "        self.layer3 = make_layer(BasicBlock, 128, 256, stride=2)\n",
    "        self.layer4 = make_layer(BasicBlock, 256, 512, stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c06beec7-f87e-4177-9718-cdb7edd13833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f955a32b-bc82-48ca-bfba-488361023b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Resnet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c09e57a9-aa60-4b75-a410-4f7c07aeca17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "             ReLU-25            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-26            [-1, 128, 4, 4]               0\n",
      "           Conv2d-27            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-28            [-1, 128, 4, 4]             256\n",
      "             ReLU-29            [-1, 128, 4, 4]               0\n",
      "           Conv2d-30            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-31            [-1, 128, 4, 4]             256\n",
      "             ReLU-32            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-33            [-1, 128, 4, 4]               0\n",
      "           Conv2d-34            [-1, 256, 2, 2]         294,912\n",
      "      BatchNorm2d-35            [-1, 256, 2, 2]             512\n",
      "             ReLU-36            [-1, 256, 2, 2]               0\n",
      "           Conv2d-37            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-38            [-1, 256, 2, 2]             512\n",
      "           Conv2d-39            [-1, 256, 2, 2]          32,768\n",
      "             ReLU-40            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-41            [-1, 256, 2, 2]               0\n",
      "           Conv2d-42            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-43            [-1, 256, 2, 2]             512\n",
      "             ReLU-44            [-1, 256, 2, 2]               0\n",
      "           Conv2d-45            [-1, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-46            [-1, 256, 2, 2]             512\n",
      "             ReLU-47            [-1, 256, 2, 2]               0\n",
      "       BasicBlock-48            [-1, 256, 2, 2]               0\n",
      "           Conv2d-49            [-1, 512, 1, 1]       1,179,648\n",
      "      BatchNorm2d-50            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-51            [-1, 512, 1, 1]               0\n",
      "           Conv2d-52            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-53            [-1, 512, 1, 1]           1,024\n",
      "           Conv2d-54            [-1, 512, 1, 1]         131,072\n",
      "             ReLU-55            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-56            [-1, 512, 1, 1]               0\n",
      "           Conv2d-57            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-58            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-59            [-1, 512, 1, 1]               0\n",
      "           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n",
      "             ReLU-62            [-1, 512, 1, 1]               0\n",
      "       BasicBlock-63            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-64            [-1, 512, 1, 1]               0\n",
      "           Linear-65                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,179,850\n",
      "Trainable params: 11,179,850\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.26\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 43.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 32, 32), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f553c32-19b3-492e-af07-f9859b1dfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a8f74f8-5911-4664-9cf1-bc6a51c61a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc6c131-ec52-4895-a926-ff7d2a47fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff639af5-48ae-420b-b764-2201dee2f50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500] Loss: 1.7437\n",
      "Epoch [1/80], Step [200/500] Loss: 1.4807\n",
      "Epoch [1/80], Step [300/500] Loss: 1.5566\n",
      "Epoch [1/80], Step [400/500] Loss: 1.5227\n",
      "Epoch [1/80], Step [500/500] Loss: 1.5672\n",
      "Epoch [2/80], Step [100/500] Loss: 1.4165\n",
      "Epoch [2/80], Step [200/500] Loss: 1.1463\n",
      "Epoch [2/80], Step [300/500] Loss: 1.2914\n",
      "Epoch [2/80], Step [400/500] Loss: 1.0318\n",
      "Epoch [2/80], Step [500/500] Loss: 1.2380\n",
      "Epoch [3/80], Step [100/500] Loss: 1.0928\n",
      "Epoch [3/80], Step [200/500] Loss: 0.8519\n",
      "Epoch [3/80], Step [300/500] Loss: 1.1353\n",
      "Epoch [3/80], Step [400/500] Loss: 0.9011\n",
      "Epoch [3/80], Step [500/500] Loss: 0.8858\n",
      "Epoch [4/80], Step [100/500] Loss: 0.7551\n",
      "Epoch [4/80], Step [200/500] Loss: 0.9546\n",
      "Epoch [4/80], Step [300/500] Loss: 1.1054\n",
      "Epoch [4/80], Step [400/500] Loss: 0.9131\n",
      "Epoch [4/80], Step [500/500] Loss: 1.0048\n",
      "Epoch [5/80], Step [100/500] Loss: 1.1182\n",
      "Epoch [5/80], Step [200/500] Loss: 0.7873\n",
      "Epoch [5/80], Step [300/500] Loss: 0.7456\n",
      "Epoch [5/80], Step [400/500] Loss: 0.7580\n",
      "Epoch [5/80], Step [500/500] Loss: 0.7603\n",
      "Epoch [6/80], Step [100/500] Loss: 0.8950\n",
      "Epoch [6/80], Step [200/500] Loss: 0.9809\n",
      "Epoch [6/80], Step [300/500] Loss: 0.8762\n",
      "Epoch [6/80], Step [400/500] Loss: 0.8486\n",
      "Epoch [6/80], Step [500/500] Loss: 0.7445\n",
      "Epoch [7/80], Step [100/500] Loss: 0.7779\n",
      "Epoch [7/80], Step [200/500] Loss: 0.7382\n",
      "Epoch [7/80], Step [300/500] Loss: 0.5952\n",
      "Epoch [7/80], Step [400/500] Loss: 0.7453\n",
      "Epoch [7/80], Step [500/500] Loss: 0.7815\n",
      "Epoch [8/80], Step [100/500] Loss: 0.6410\n",
      "Epoch [8/80], Step [200/500] Loss: 0.8150\n",
      "Epoch [8/80], Step [300/500] Loss: 0.7446\n",
      "Epoch [8/80], Step [400/500] Loss: 0.5039\n",
      "Epoch [8/80], Step [500/500] Loss: 0.6476\n",
      "Epoch [9/80], Step [100/500] Loss: 0.7998\n",
      "Epoch [9/80], Step [200/500] Loss: 0.7817\n",
      "Epoch [9/80], Step [300/500] Loss: 0.5644\n",
      "Epoch [9/80], Step [400/500] Loss: 0.5979\n",
      "Epoch [9/80], Step [500/500] Loss: 0.5907\n",
      "Epoch [10/80], Step [100/500] Loss: 0.5932\n",
      "Epoch [10/80], Step [200/500] Loss: 0.6427\n",
      "Epoch [10/80], Step [300/500] Loss: 0.5625\n",
      "Epoch [10/80], Step [400/500] Loss: 0.5059\n",
      "Epoch [10/80], Step [500/500] Loss: 0.4698\n",
      "Epoch [11/80], Step [100/500] Loss: 0.6064\n",
      "Epoch [11/80], Step [200/500] Loss: 0.6820\n",
      "Epoch [11/80], Step [300/500] Loss: 0.4890\n",
      "Epoch [11/80], Step [400/500] Loss: 0.6082\n",
      "Epoch [11/80], Step [500/500] Loss: 0.5432\n",
      "Epoch [12/80], Step [100/500] Loss: 0.5098\n",
      "Epoch [12/80], Step [200/500] Loss: 0.6789\n",
      "Epoch [12/80], Step [300/500] Loss: 0.4559\n",
      "Epoch [12/80], Step [400/500] Loss: 0.5910\n",
      "Epoch [12/80], Step [500/500] Loss: 0.4918\n",
      "Epoch [13/80], Step [100/500] Loss: 0.4899\n",
      "Epoch [13/80], Step [200/500] Loss: 0.6481\n",
      "Epoch [13/80], Step [300/500] Loss: 0.5417\n",
      "Epoch [13/80], Step [400/500] Loss: 0.4185\n",
      "Epoch [13/80], Step [500/500] Loss: 0.6123\n",
      "Epoch [14/80], Step [100/500] Loss: 0.6054\n",
      "Epoch [14/80], Step [200/500] Loss: 0.5304\n",
      "Epoch [14/80], Step [300/500] Loss: 0.4495\n",
      "Epoch [14/80], Step [400/500] Loss: 0.4445\n",
      "Epoch [14/80], Step [500/500] Loss: 0.5840\n",
      "Epoch [15/80], Step [100/500] Loss: 0.5389\n",
      "Epoch [15/80], Step [200/500] Loss: 0.5225\n",
      "Epoch [15/80], Step [300/500] Loss: 0.4613\n",
      "Epoch [15/80], Step [400/500] Loss: 0.5664\n",
      "Epoch [15/80], Step [500/500] Loss: 0.6790\n",
      "Epoch [16/80], Step [100/500] Loss: 0.5925\n",
      "Epoch [16/80], Step [200/500] Loss: 0.5489\n",
      "Epoch [16/80], Step [300/500] Loss: 0.4557\n",
      "Epoch [16/80], Step [400/500] Loss: 0.5443\n",
      "Epoch [16/80], Step [500/500] Loss: 0.5139\n",
      "Epoch [17/80], Step [100/500] Loss: 0.4401\n",
      "Epoch [17/80], Step [200/500] Loss: 0.4329\n",
      "Epoch [17/80], Step [300/500] Loss: 0.6129\n",
      "Epoch [17/80], Step [400/500] Loss: 0.4652\n",
      "Epoch [17/80], Step [500/500] Loss: 0.5336\n",
      "Epoch [18/80], Step [100/500] Loss: 0.4133\n",
      "Epoch [18/80], Step [200/500] Loss: 0.5482\n",
      "Epoch [18/80], Step [300/500] Loss: 0.5291\n",
      "Epoch [18/80], Step [400/500] Loss: 0.3714\n",
      "Epoch [18/80], Step [500/500] Loss: 0.4806\n",
      "Epoch [19/80], Step [100/500] Loss: 0.4473\n",
      "Epoch [19/80], Step [200/500] Loss: 0.4114\n",
      "Epoch [19/80], Step [300/500] Loss: 0.4667\n",
      "Epoch [19/80], Step [400/500] Loss: 0.3699\n",
      "Epoch [19/80], Step [500/500] Loss: 0.4811\n",
      "Epoch [20/80], Step [100/500] Loss: 0.4446\n",
      "Epoch [20/80], Step [200/500] Loss: 0.4118\n",
      "Epoch [20/80], Step [300/500] Loss: 0.4272\n",
      "Epoch [20/80], Step [400/500] Loss: 0.4419\n",
      "Epoch [20/80], Step [500/500] Loss: 0.6132\n",
      "Epoch [21/80], Step [100/500] Loss: 0.2983\n",
      "Epoch [21/80], Step [200/500] Loss: 0.3199\n",
      "Epoch [21/80], Step [300/500] Loss: 0.5114\n",
      "Epoch [21/80], Step [400/500] Loss: 0.3550\n",
      "Epoch [21/80], Step [500/500] Loss: 0.5154\n",
      "Epoch [22/80], Step [100/500] Loss: 0.3458\n",
      "Epoch [22/80], Step [200/500] Loss: 0.2050\n",
      "Epoch [22/80], Step [300/500] Loss: 0.3171\n",
      "Epoch [22/80], Step [400/500] Loss: 0.3108\n",
      "Epoch [22/80], Step [500/500] Loss: 0.2445\n",
      "Epoch [23/80], Step [100/500] Loss: 0.1819\n",
      "Epoch [23/80], Step [200/500] Loss: 0.3558\n",
      "Epoch [23/80], Step [300/500] Loss: 0.3247\n",
      "Epoch [23/80], Step [400/500] Loss: 0.3516\n",
      "Epoch [23/80], Step [500/500] Loss: 0.3353\n",
      "Epoch [24/80], Step [100/500] Loss: 0.3368\n",
      "Epoch [24/80], Step [200/500] Loss: 0.4251\n",
      "Epoch [24/80], Step [300/500] Loss: 0.2563\n",
      "Epoch [24/80], Step [400/500] Loss: 0.2577\n",
      "Epoch [24/80], Step [500/500] Loss: 0.4070\n",
      "Epoch [25/80], Step [100/500] Loss: 0.3223\n",
      "Epoch [25/80], Step [200/500] Loss: 0.3115\n",
      "Epoch [25/80], Step [300/500] Loss: 0.2458\n",
      "Epoch [25/80], Step [400/500] Loss: 0.4133\n",
      "Epoch [25/80], Step [500/500] Loss: 0.2918\n",
      "Epoch [26/80], Step [100/500] Loss: 0.2890\n",
      "Epoch [26/80], Step [200/500] Loss: 0.3200\n",
      "Epoch [26/80], Step [300/500] Loss: 0.2625\n",
      "Epoch [26/80], Step [400/500] Loss: 0.2442\n",
      "Epoch [26/80], Step [500/500] Loss: 0.3893\n",
      "Epoch [27/80], Step [100/500] Loss: 0.1769\n",
      "Epoch [27/80], Step [200/500] Loss: 0.2746\n",
      "Epoch [27/80], Step [300/500] Loss: 0.2381\n",
      "Epoch [27/80], Step [400/500] Loss: 0.3891\n",
      "Epoch [27/80], Step [500/500] Loss: 0.3039\n",
      "Epoch [28/80], Step [100/500] Loss: 0.2940\n",
      "Epoch [28/80], Step [200/500] Loss: 0.3284\n",
      "Epoch [28/80], Step [300/500] Loss: 0.2309\n",
      "Epoch [28/80], Step [400/500] Loss: 0.2667\n",
      "Epoch [28/80], Step [500/500] Loss: 0.3493\n",
      "Epoch [29/80], Step [100/500] Loss: 0.2670\n",
      "Epoch [29/80], Step [200/500] Loss: 0.1545\n",
      "Epoch [29/80], Step [300/500] Loss: 0.2724\n",
      "Epoch [29/80], Step [400/500] Loss: 0.2824\n",
      "Epoch [29/80], Step [500/500] Loss: 0.2837\n",
      "Epoch [30/80], Step [100/500] Loss: 0.3231\n",
      "Epoch [30/80], Step [200/500] Loss: 0.3425\n",
      "Epoch [30/80], Step [300/500] Loss: 0.2968\n",
      "Epoch [30/80], Step [400/500] Loss: 0.2744\n",
      "Epoch [30/80], Step [500/500] Loss: 0.3559\n",
      "Epoch [31/80], Step [100/500] Loss: 0.2116\n",
      "Epoch [31/80], Step [200/500] Loss: 0.3874\n",
      "Epoch [31/80], Step [300/500] Loss: 0.2447\n",
      "Epoch [31/80], Step [400/500] Loss: 0.2842\n",
      "Epoch [31/80], Step [500/500] Loss: 0.3409\n",
      "Epoch [32/80], Step [100/500] Loss: 0.3119\n",
      "Epoch [32/80], Step [200/500] Loss: 0.3527\n",
      "Epoch [32/80], Step [300/500] Loss: 0.1600\n",
      "Epoch [32/80], Step [400/500] Loss: 0.2588\n",
      "Epoch [32/80], Step [500/500] Loss: 0.2576\n",
      "Epoch [33/80], Step [100/500] Loss: 0.2255\n",
      "Epoch [33/80], Step [200/500] Loss: 0.1334\n",
      "Epoch [33/80], Step [300/500] Loss: 0.1873\n",
      "Epoch [33/80], Step [400/500] Loss: 0.2707\n",
      "Epoch [33/80], Step [500/500] Loss: 0.2689\n",
      "Epoch [34/80], Step [100/500] Loss: 0.3526\n",
      "Epoch [34/80], Step [200/500] Loss: 0.3114\n",
      "Epoch [34/80], Step [300/500] Loss: 0.1532\n",
      "Epoch [34/80], Step [400/500] Loss: 0.2338\n",
      "Epoch [34/80], Step [500/500] Loss: 0.3797\n",
      "Epoch [35/80], Step [100/500] Loss: 0.3313\n",
      "Epoch [35/80], Step [200/500] Loss: 0.1670\n",
      "Epoch [35/80], Step [300/500] Loss: 0.4260\n",
      "Epoch [35/80], Step [400/500] Loss: 0.2286\n",
      "Epoch [35/80], Step [500/500] Loss: 0.2378\n",
      "Epoch [36/80], Step [100/500] Loss: 0.2203\n",
      "Epoch [36/80], Step [200/500] Loss: 0.2410\n",
      "Epoch [36/80], Step [300/500] Loss: 0.2757\n",
      "Epoch [36/80], Step [400/500] Loss: 0.3545\n",
      "Epoch [36/80], Step [500/500] Loss: 0.1834\n",
      "Epoch [37/80], Step [100/500] Loss: 0.1890\n",
      "Epoch [37/80], Step [200/500] Loss: 0.2457\n",
      "Epoch [37/80], Step [300/500] Loss: 0.1767\n",
      "Epoch [37/80], Step [400/500] Loss: 0.2560\n",
      "Epoch [37/80], Step [500/500] Loss: 0.2012\n",
      "Epoch [38/80], Step [100/500] Loss: 0.3033\n",
      "Epoch [38/80], Step [200/500] Loss: 0.1426\n",
      "Epoch [38/80], Step [300/500] Loss: 0.1417\n",
      "Epoch [38/80], Step [400/500] Loss: 0.2452\n",
      "Epoch [38/80], Step [500/500] Loss: 0.3308\n",
      "Epoch [39/80], Step [100/500] Loss: 0.2568\n",
      "Epoch [39/80], Step [200/500] Loss: 0.2170\n",
      "Epoch [39/80], Step [300/500] Loss: 0.2961\n",
      "Epoch [39/80], Step [400/500] Loss: 0.1772\n",
      "Epoch [39/80], Step [500/500] Loss: 0.2366\n",
      "Epoch [40/80], Step [100/500] Loss: 0.2423\n",
      "Epoch [40/80], Step [200/500] Loss: 0.1844\n",
      "Epoch [40/80], Step [300/500] Loss: 0.3012\n",
      "Epoch [40/80], Step [400/500] Loss: 0.2356\n",
      "Epoch [40/80], Step [500/500] Loss: 0.1724\n",
      "Epoch [41/80], Step [100/500] Loss: 0.2769\n",
      "Epoch [41/80], Step [200/500] Loss: 0.2340\n",
      "Epoch [41/80], Step [300/500] Loss: 0.1455\n",
      "Epoch [41/80], Step [400/500] Loss: 0.1958\n",
      "Epoch [41/80], Step [500/500] Loss: 0.2023\n",
      "Epoch [42/80], Step [100/500] Loss: 0.1999\n",
      "Epoch [42/80], Step [200/500] Loss: 0.1229\n",
      "Epoch [42/80], Step [300/500] Loss: 0.2123\n",
      "Epoch [42/80], Step [400/500] Loss: 0.1907\n",
      "Epoch [42/80], Step [500/500] Loss: 0.2209\n",
      "Epoch [43/80], Step [100/500] Loss: 0.1943\n",
      "Epoch [43/80], Step [200/500] Loss: 0.2125\n",
      "Epoch [43/80], Step [300/500] Loss: 0.1246\n",
      "Epoch [43/80], Step [400/500] Loss: 0.2537\n",
      "Epoch [43/80], Step [500/500] Loss: 0.2141\n",
      "Epoch [44/80], Step [100/500] Loss: 0.2976\n",
      "Epoch [44/80], Step [200/500] Loss: 0.1055\n",
      "Epoch [44/80], Step [300/500] Loss: 0.1896\n",
      "Epoch [44/80], Step [400/500] Loss: 0.2973\n",
      "Epoch [44/80], Step [500/500] Loss: 0.1212\n",
      "Epoch [45/80], Step [100/500] Loss: 0.0643\n",
      "Epoch [45/80], Step [200/500] Loss: 0.2203\n",
      "Epoch [45/80], Step [300/500] Loss: 0.4315\n",
      "Epoch [45/80], Step [400/500] Loss: 0.3049\n",
      "Epoch [45/80], Step [500/500] Loss: 0.2789\n",
      "Epoch [46/80], Step [100/500] Loss: 0.1293\n",
      "Epoch [46/80], Step [200/500] Loss: 0.1847\n",
      "Epoch [46/80], Step [300/500] Loss: 0.2346\n",
      "Epoch [46/80], Step [400/500] Loss: 0.1473\n",
      "Epoch [46/80], Step [500/500] Loss: 0.1843\n",
      "Epoch [47/80], Step [100/500] Loss: 0.1037\n",
      "Epoch [47/80], Step [200/500] Loss: 0.1792\n",
      "Epoch [47/80], Step [300/500] Loss: 0.1959\n",
      "Epoch [47/80], Step [400/500] Loss: 0.1175\n",
      "Epoch [47/80], Step [500/500] Loss: 0.0817\n",
      "Epoch [48/80], Step [100/500] Loss: 0.1798\n",
      "Epoch [48/80], Step [200/500] Loss: 0.1485\n",
      "Epoch [48/80], Step [300/500] Loss: 0.1431\n",
      "Epoch [48/80], Step [400/500] Loss: 0.1836\n",
      "Epoch [48/80], Step [500/500] Loss: 0.2139\n",
      "Epoch [49/80], Step [100/500] Loss: 0.2493\n",
      "Epoch [49/80], Step [200/500] Loss: 0.1074\n",
      "Epoch [49/80], Step [300/500] Loss: 0.0745\n",
      "Epoch [49/80], Step [400/500] Loss: 0.1425\n",
      "Epoch [49/80], Step [500/500] Loss: 0.1479\n",
      "Epoch [50/80], Step [100/500] Loss: 0.1078\n",
      "Epoch [50/80], Step [200/500] Loss: 0.0919\n",
      "Epoch [50/80], Step [300/500] Loss: 0.1177\n",
      "Epoch [50/80], Step [400/500] Loss: 0.1725\n",
      "Epoch [50/80], Step [500/500] Loss: 0.2124\n",
      "Epoch [51/80], Step [100/500] Loss: 0.1608\n",
      "Epoch [51/80], Step [200/500] Loss: 0.1470\n",
      "Epoch [51/80], Step [300/500] Loss: 0.1057\n",
      "Epoch [51/80], Step [400/500] Loss: 0.1720\n",
      "Epoch [51/80], Step [500/500] Loss: 0.0878\n",
      "Epoch [52/80], Step [100/500] Loss: 0.1733\n",
      "Epoch [52/80], Step [200/500] Loss: 0.2544\n",
      "Epoch [52/80], Step [300/500] Loss: 0.1657\n",
      "Epoch [52/80], Step [400/500] Loss: 0.2377\n",
      "Epoch [52/80], Step [500/500] Loss: 0.1147\n",
      "Epoch [53/80], Step [100/500] Loss: 0.1713\n",
      "Epoch [53/80], Step [200/500] Loss: 0.1267\n",
      "Epoch [53/80], Step [300/500] Loss: 0.1844\n",
      "Epoch [53/80], Step [400/500] Loss: 0.1810\n",
      "Epoch [53/80], Step [500/500] Loss: 0.1136\n",
      "Epoch [54/80], Step [100/500] Loss: 0.1001\n",
      "Epoch [54/80], Step [200/500] Loss: 0.1100\n",
      "Epoch [54/80], Step [300/500] Loss: 0.1959\n",
      "Epoch [54/80], Step [400/500] Loss: 0.3652\n",
      "Epoch [54/80], Step [500/500] Loss: 0.1408\n",
      "Epoch [55/80], Step [100/500] Loss: 0.1994\n",
      "Epoch [55/80], Step [200/500] Loss: 0.0934\n",
      "Epoch [55/80], Step [300/500] Loss: 0.1407\n",
      "Epoch [55/80], Step [400/500] Loss: 0.2219\n",
      "Epoch [55/80], Step [500/500] Loss: 0.1375\n",
      "Epoch [56/80], Step [100/500] Loss: 0.0903\n",
      "Epoch [56/80], Step [200/500] Loss: 0.2297\n",
      "Epoch [56/80], Step [300/500] Loss: 0.1323\n",
      "Epoch [56/80], Step [400/500] Loss: 0.1096\n",
      "Epoch [56/80], Step [500/500] Loss: 0.1668\n",
      "Epoch [57/80], Step [100/500] Loss: 0.1463\n",
      "Epoch [57/80], Step [200/500] Loss: 0.1413\n",
      "Epoch [57/80], Step [300/500] Loss: 0.1547\n",
      "Epoch [57/80], Step [400/500] Loss: 0.1915\n",
      "Epoch [57/80], Step [500/500] Loss: 0.2034\n",
      "Epoch [58/80], Step [100/500] Loss: 0.0843\n",
      "Epoch [58/80], Step [200/500] Loss: 0.1098\n",
      "Epoch [58/80], Step [300/500] Loss: 0.1277\n",
      "Epoch [58/80], Step [400/500] Loss: 0.2232\n",
      "Epoch [58/80], Step [500/500] Loss: 0.1382\n",
      "Epoch [59/80], Step [100/500] Loss: 0.1199\n",
      "Epoch [59/80], Step [200/500] Loss: 0.1357\n",
      "Epoch [59/80], Step [300/500] Loss: 0.0938\n",
      "Epoch [59/80], Step [400/500] Loss: 0.1719\n",
      "Epoch [59/80], Step [500/500] Loss: 0.1004\n",
      "Epoch [60/80], Step [100/500] Loss: 0.1049\n",
      "Epoch [60/80], Step [200/500] Loss: 0.2024\n",
      "Epoch [60/80], Step [300/500] Loss: 0.1554\n",
      "Epoch [60/80], Step [400/500] Loss: 0.0858\n",
      "Epoch [60/80], Step [500/500] Loss: 0.1363\n",
      "Epoch [61/80], Step [100/500] Loss: 0.1943\n",
      "Epoch [61/80], Step [200/500] Loss: 0.0836\n",
      "Epoch [61/80], Step [300/500] Loss: 0.1328\n",
      "Epoch [61/80], Step [400/500] Loss: 0.1440\n",
      "Epoch [61/80], Step [500/500] Loss: 0.1357\n",
      "Epoch [62/80], Step [100/500] Loss: 0.1058\n",
      "Epoch [62/80], Step [200/500] Loss: 0.0819\n",
      "Epoch [62/80], Step [300/500] Loss: 0.3476\n",
      "Epoch [62/80], Step [400/500] Loss: 0.2725\n",
      "Epoch [62/80], Step [500/500] Loss: 0.0436\n",
      "Epoch [63/80], Step [100/500] Loss: 0.1612\n",
      "Epoch [63/80], Step [200/500] Loss: 0.1094\n",
      "Epoch [63/80], Step [300/500] Loss: 0.1242\n",
      "Epoch [63/80], Step [400/500] Loss: 0.2716\n",
      "Epoch [63/80], Step [500/500] Loss: 0.1573\n",
      "Epoch [64/80], Step [100/500] Loss: 0.1068\n",
      "Epoch [64/80], Step [200/500] Loss: 0.0850\n",
      "Epoch [64/80], Step [300/500] Loss: 0.1668\n",
      "Epoch [64/80], Step [400/500] Loss: 0.1972\n",
      "Epoch [64/80], Step [500/500] Loss: 0.0637\n",
      "Epoch [65/80], Step [100/500] Loss: 0.1268\n",
      "Epoch [65/80], Step [200/500] Loss: 0.0951\n",
      "Epoch [65/80], Step [300/500] Loss: 0.1025\n",
      "Epoch [65/80], Step [400/500] Loss: 0.1787\n",
      "Epoch [65/80], Step [500/500] Loss: 0.0936\n",
      "Epoch [66/80], Step [100/500] Loss: 0.1577\n",
      "Epoch [66/80], Step [200/500] Loss: 0.1613\n",
      "Epoch [66/80], Step [300/500] Loss: 0.1560\n",
      "Epoch [66/80], Step [400/500] Loss: 0.0736\n",
      "Epoch [66/80], Step [500/500] Loss: 0.1227\n",
      "Epoch [67/80], Step [100/500] Loss: 0.1825\n",
      "Epoch [67/80], Step [200/500] Loss: 0.0817\n",
      "Epoch [67/80], Step [300/500] Loss: 0.2050\n",
      "Epoch [67/80], Step [400/500] Loss: 0.0935\n",
      "Epoch [67/80], Step [500/500] Loss: 0.0895\n",
      "Epoch [68/80], Step [100/500] Loss: 0.1386\n",
      "Epoch [68/80], Step [200/500] Loss: 0.1172\n",
      "Epoch [68/80], Step [300/500] Loss: 0.1511\n",
      "Epoch [68/80], Step [400/500] Loss: 0.0980\n",
      "Epoch [68/80], Step [500/500] Loss: 0.0528\n",
      "Epoch [69/80], Step [100/500] Loss: 0.1007\n",
      "Epoch [69/80], Step [200/500] Loss: 0.1258\n",
      "Epoch [69/80], Step [300/500] Loss: 0.1207\n",
      "Epoch [69/80], Step [400/500] Loss: 0.1060\n",
      "Epoch [69/80], Step [500/500] Loss: 0.2199\n",
      "Epoch [70/80], Step [100/500] Loss: 0.1390\n",
      "Epoch [70/80], Step [200/500] Loss: 0.1575\n",
      "Epoch [70/80], Step [300/500] Loss: 0.0888\n",
      "Epoch [70/80], Step [400/500] Loss: 0.1230\n",
      "Epoch [70/80], Step [500/500] Loss: 0.1523\n",
      "Epoch [71/80], Step [100/500] Loss: 0.2097\n",
      "Epoch [71/80], Step [200/500] Loss: 0.1966\n",
      "Epoch [71/80], Step [300/500] Loss: 0.1485\n",
      "Epoch [71/80], Step [400/500] Loss: 0.1535\n",
      "Epoch [71/80], Step [500/500] Loss: 0.0997\n",
      "Epoch [72/80], Step [100/500] Loss: 0.1239\n",
      "Epoch [72/80], Step [200/500] Loss: 0.1233\n",
      "Epoch [72/80], Step [300/500] Loss: 0.0612\n",
      "Epoch [72/80], Step [400/500] Loss: 0.1000\n",
      "Epoch [72/80], Step [500/500] Loss: 0.2156\n",
      "Epoch [73/80], Step [100/500] Loss: 0.1792\n",
      "Epoch [73/80], Step [200/500] Loss: 0.1251\n",
      "Epoch [73/80], Step [300/500] Loss: 0.1545\n",
      "Epoch [73/80], Step [400/500] Loss: 0.1032\n",
      "Epoch [73/80], Step [500/500] Loss: 0.1034\n",
      "Epoch [74/80], Step [100/500] Loss: 0.0388\n",
      "Epoch [74/80], Step [200/500] Loss: 0.1863\n",
      "Epoch [74/80], Step [300/500] Loss: 0.1156\n",
      "Epoch [74/80], Step [400/500] Loss: 0.0611\n",
      "Epoch [74/80], Step [500/500] Loss: 0.1341\n",
      "Epoch [75/80], Step [100/500] Loss: 0.0570\n",
      "Epoch [75/80], Step [200/500] Loss: 0.0808\n",
      "Epoch [75/80], Step [300/500] Loss: 0.1214\n",
      "Epoch [75/80], Step [400/500] Loss: 0.1742\n",
      "Epoch [75/80], Step [500/500] Loss: 0.0972\n",
      "Epoch [76/80], Step [100/500] Loss: 0.1355\n",
      "Epoch [76/80], Step [200/500] Loss: 0.1940\n",
      "Epoch [76/80], Step [300/500] Loss: 0.0914\n",
      "Epoch [76/80], Step [400/500] Loss: 0.2545\n",
      "Epoch [76/80], Step [500/500] Loss: 0.0826\n",
      "Epoch [77/80], Step [100/500] Loss: 0.2023\n",
      "Epoch [77/80], Step [200/500] Loss: 0.0705\n",
      "Epoch [77/80], Step [300/500] Loss: 0.0873\n",
      "Epoch [77/80], Step [400/500] Loss: 0.1062\n",
      "Epoch [77/80], Step [500/500] Loss: 0.1281\n",
      "Epoch [78/80], Step [100/500] Loss: 0.1187\n",
      "Epoch [78/80], Step [200/500] Loss: 0.1258\n",
      "Epoch [78/80], Step [300/500] Loss: 0.0332\n",
      "Epoch [78/80], Step [400/500] Loss: 0.0994\n",
      "Epoch [78/80], Step [500/500] Loss: 0.1354\n",
      "Epoch [79/80], Step [100/500] Loss: 0.0552\n",
      "Epoch [79/80], Step [200/500] Loss: 0.1175\n",
      "Epoch [79/80], Step [300/500] Loss: 0.1693\n",
      "Epoch [79/80], Step [400/500] Loss: 0.1000\n",
      "Epoch [79/80], Step [500/500] Loss: 0.1382\n",
      "Epoch [80/80], Step [100/500] Loss: 0.0536\n",
      "Epoch [80/80], Step [200/500] Loss: 0.0525\n",
      "Epoch [80/80], Step [300/500] Loss: 0.0583\n",
      "Epoch [80/80], Step [400/500] Loss: 0.0624\n",
      "Epoch [80/80], Step [500/500] Loss: 0.0634\n"
     ]
    }
   ],
   "source": [
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012ed83-4962-4fd3-83a1-10a733971145",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
